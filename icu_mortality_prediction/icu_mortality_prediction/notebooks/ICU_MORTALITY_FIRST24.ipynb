{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "import numpy as np\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import psycopg2\n",
    "from scipy.stats import ks_2samp\n",
    "import scipy.stats as scats\n",
    "#from scipy.stats import boxcox\n",
    "# import visuals as vs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn import metrics\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from heapq import nlargest\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "# import cPickle as pickle\n",
    "from icu_mortality_prediction import DATA_DIR\n",
    "\n",
    "plt.style.use('ggplot') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MIMIC-III Critical Care Database\n",
    "\n",
    "MIMIC-III (Medical Information Mart for Intensive Care III) is a large, freely-available database comprising deidentified health-related data associated with over forty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center between 2001 and 2012.\n",
    "\n",
    "The database includes information such as demographics, vital sign measurements made at the bedside (~1 data point per hour), laboratory test results, procedures, medications, caregiver notes, imaging reports, and mortality (both in and out of hospital).\n",
    "\n",
    "MIMIC supports a diverse range of analytic studies spanning epidemiology, clinical decision-rule improvement, and electronic tool development. It is notable for three factors:\n",
    "\n",
    "it is freely available to researchers worldwide\n",
    "it encompasses a diverse and very large population of ICU patients\n",
    "it contains high temporal resolution data including lab results, electronic documentation, and bedside monitor trends and waveforms.\n",
    "\n",
    "Citations: \n",
    "MIMIC-III, a freely accessible critical care database. Johnson AEW, Pollard TJ, Shen L, Lehman L, Feng M, Ghassemi M, Moody B, Szolovits P, Celi LA, and Mark RG. Scientific Data (2016). DOI: 10.1038/sdata.2016.35. Available at: http://www.nature.com/articles/sdata201635\n",
    "\n",
    "Pollard, T. J. & Johnson, A. E. W. The MIMIC-III Clinical Database http://dx.doi.org/10.13026/C2XW26 (2016).\n",
    "\n",
    "\n",
    "# IMPORTING DATA\n",
    "The mimic III database was downloaded and reconstructed locally using posgresql. The database was managed graphically using Portico. \n",
    "A query was run on the mimic III database to generate demographic data and data concerning hospital and ICU stays for patients diagnosed with sepsis. The query was exported from Porticoto the file ADMISSIONS_ICUSTAY_SEPSIS.csv. The data was read into a pandas dataframe ptnt_demog \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTEGRATE DATA FROM LABS AND CHARTS\n",
    "Data from labs and charts have been previously aggregated and summarized (variables with multiple measurements have had mean, std, skewness etc calculated). Data is integrated/merged into a single data_frame. \n",
    "\n",
    "## FEATURE SELECTION\n",
    "Feature selection tools, including SelectKBest, require data sets to be full (no NaN values)and to be of the same type (continuous/categorical). To reduce the dimensionality of our data and select the most relevant features, data is then broken out into blocks according to data type. Because not all data was collected for all icu stays or patients there are gaps such that there are very few icu stays for which there are no NaN values. There were patterns of data collection however, groups of variables that were most often collected together (likely the results of the condition, the type of ICU etc). To generate full sets of data for feature selection, the data is further broken down into blocks of variables most often collected together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_features = 30\n",
    "\n",
    "\n",
    "# patient demographic data includes diagnoses and icd9 codes for each patient and each icustay\n",
    "features_dirname = 'features'\n",
    "features_dirpath = os.path.join(DATA_DIR, 'processed',features_dirname)\n",
    "\n",
    "# features_dir = os.getcwd() + '/features/'\n",
    "#display(features_dir)\n",
    "monkey = os.listdir(features_dirpath)\n",
    "feature_files = [x for x in monkey if '.csv' in x]\n",
    "features_dict = {}\n",
    "for name in feature_files:\n",
    "    print(name)\n",
    "    features_dict[name.split('.')[0]] = pd.read_csv(os.path.join(features_dirpath, name), \n",
    "                                                   index_col=0)\n",
    "outcomes = pd.read_csv(os.path.join(features_dirpath, 'outcomes.csv'), \n",
    "                                                    index_col=0)\n",
    "\n",
    "scores = [x for x in features_dict.keys() if 'Scores' in x]\n",
    "features = [x for x in features_dict.keys() if 'Scores' not in x]\n",
    "scores.sort()\n",
    "features.sort()\n",
    "\n",
    "\n",
    "for i in range(len(scores)):\n",
    "    if i == 0:\n",
    "        all_scores = features_dict[scores[i]]\n",
    "    else:\n",
    "        all_scores = all_scores.append(features_dict[scores[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = all_scores.sort_values(by = 'p_values', ascending = True)\n",
    "all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_scores = list(all_scores.head(num_features).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(\"all scores\")\n",
    "# display(all_scores)\n",
    "# display(\"top scores\")\n",
    "# display(top_scores)\n",
    "\n",
    "first = True\n",
    "all_data = outcomes\n",
    "for frame in features:\n",
    "    for col in features_dict[frame].columns:\n",
    "        if col in top_scores:\n",
    "            \n",
    "            feat = features_dict[frame][col] \n",
    "            all_data = all_data.merge(pd.DataFrame(feat), left_index = True, \n",
    "                                     right_index = True, how = 'outer', sort = True)\n",
    "            \n",
    "            #display(feat.name)\n",
    "            #display(all_data.shape)\n",
    "            \n",
    "# all_data.rename(index=str, columns={\"Pneumonia (except that caused by tuberculosis or sexually transmitted disease)\": \"Pneumonia\",\n",
    "#                                          \"Respiratory failure; insufficiency; arrest (adult)\": \"Respiratory Failure\"}, \n",
    "#                      inplace = True)\n",
    "\n",
    "\n",
    "display(all_data.shape)\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = all_data[all_data.columns[1:]]\n",
    "# imp_iter = IterativeImputer(missing_values = np.nan, max_iter=100, add_indicator=True)\n",
    "imp_iter = SimpleImputer(missing_values = np.nan, add_indicator=True)\n",
    "imp_iter.fit(X)\n",
    "imputed_data_df = imp_iter.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.DataFrame([[np.nan, True, False], [True, False, True],  [False, True, np.nan]], \n",
    "#                 columns=['a', 'b', 'c'],)\n",
    "\n",
    "# # imp_iter = SimpleImputer(missing_values = np.nan, max_iter=10, add_indicator=True)\n",
    "# imp_iter = SimpleImputer(missing_values = np.nan,  add_indicator=True)\n",
    "# imp_iter.fit(X)\n",
    "# imputed_data = imp_iter.transform(X)\n",
    "\n",
    "# imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame in features:\n",
    "    for col in features_dict[frame].columns:\n",
    "        if col in top_scores:\n",
    "            print(col)\n",
    "            feat = features_dict[frame][col] \n",
    "            feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = all_data.merge(pd.DataFrame(feat), left_index = True, \n",
    "                                     right_index = True, how = 'inner', sort = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for frame in features:\n",
    "    for col in features_dict[frame].columns:\n",
    "        if col in top_scores:\n",
    "            print(col)\n",
    "            feat = features_dict[frame][col] \n",
    "            all_data = all_data.merge(pd.DataFrame(feat), left_index = True, \n",
    "                                     right_index = True, how = 'inner', sort = True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = outcomes\n",
    "for frame in features:\n",
    "    for col in features_dict[frame].columns:\n",
    "        if col in top_scores:\n",
    "            \n",
    "            feat = features_dict[frame][col] \n",
    "            all_data = all_data.merge(pd.DataFrame(feat), left_index = True, \n",
    "                                     right_index = True, how = 'inner', sort = True)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cols = list(all_data.columns)\n",
    "#cols.sort()\n",
    "for col in cols:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "diagnoses_list = ['hospital_expire_flag', 'GCS Total_15', 'GCS Total_3', 'GCS Total_6', 'GCS Total_4', \n",
    "                          'Respiratory Failure','Shock', 'Septicemia (except in labor)', 'Acute and unspecified renal failure','Other liver diseases',\n",
    "                         'Fluid and electrolyte disorders',\n",
    "                          'Pneumonia',\n",
    "                            'Acute cerebrovascular disease',]\n",
    "meas_list = ['hospital_expire_flag', 'Creat_abnflag','icu_stay_Q3','first_careunit_CSRU',\n",
    "                          'admission_type_ELECTIVE', 'icu_stay_Q0','age_Q3','first_careunit_MICU']\n",
    "diagnoses_features = all_data[diagnoses_list]\n",
    "meas_features = all_data[meas_list]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display(diagnoses_features[diagnoses_features.hospital_expire_flag == 1].shape)\n",
    "monkey = diagnoses_features.groupby('hospital_expire_flag').sum()\n",
    "\n",
    "monkey.append(diagnoses_features.groupby('hospital_expire_flag').sum()/diagnoses_features.groupby('hospital_expire_flag').count()).transpose()\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for feat in diagnoses_features.columns:\n",
    "    print(feat)\n",
    "print \"****************************\"\n",
    "for feat in meas_features.columns:\n",
    "    print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE GRAPHS ILLUSTRATING SURVIVAL RATES FOR PATIENTS IN WHICH FEATURE OR CONDITION IS TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "live_dead_dict = {}\n",
    "col = all_data.columns[1]\n",
    "\n",
    "for col in all_data.columns[1:]: #all_data.columns[1:]:\n",
    "    # ALL DATA FOR WHICH \n",
    "    positives = all_data[all_data[col] == 1] #all_data[all_data[col] == 1]\n",
    "    dead = positives.hospital_expire_flag.sum()\n",
    "    total = positives.hospital_expire_flag.count()\n",
    "    dead_percent = 100.0*dead/total\n",
    "    live_percent = 100.0*(total-dead)/total\n",
    "    live_dead_dict[col] = [dead_percent, live_percent]\n",
    "    \n",
    "live_dead_df =pd.DataFrame.from_dict(live_dead_dict)\n",
    "live_dead_df.index = [['Non-Survivors', 'Survivors']]\n",
    "#display(live_dead_df[diagnoses_list[1:]])\n",
    "live_dead_df[diagnoses_list[1:]].transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors and Non-Survivors for \\n ICU Stays where Each Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "#display(live_dead_df[meas_list[1:]])\n",
    "live_dead_df[meas_list[1:]].transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors of Non-Survivors for  \\n  ICU Stays where Each Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "live_dead_dict = {}\n",
    "col = diagnoses_features.columns[1]\n",
    "\n",
    "for col in diagnoses_features.columns[1:]: #all_data.columns[1:]:\n",
    "    # ALL DATA FOR WHICH \n",
    "    positives = diagnoses_features[diagnoses_features[col] == 1] #all_data[all_data[col] == 1]\n",
    "    dead = positives.hospital_expire_flag.sum()\n",
    "    total = positives.hospital_expire_flag.count()\n",
    "    dead_percent = 100.0*dead/total\n",
    "    live_percent = 100.0*(total-dead)/total\n",
    "    live_dead_dict[col] = [dead_percent, live_percent]\n",
    "    \n",
    "live_dead_df =pd.DataFrame.from_dict(live_dead_dict)\n",
    "live_dead_df.index = [['Non-Survivors', 'Survivors']]\n",
    "#display(live_dead_df[diagnoses_list[1:]])\n",
    "live_dead_df.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors and Non-Survivors for \\n ICU Stays where Each Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "\n",
    "'''\n",
    "#display(live_dead_df[meas_list[1:]])\n",
    "live_dead_df[meas_list[1:]].transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors of Non-Survivors for  \\n  ICU Stays where Each Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(live_dead_df/100).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE GRAPHS ILLUSTRATING THE RATES OF TRUE VALUES IN SURVIVAL AND NON-SURVIVAL GROUPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dead_positive_dict = {}\n",
    "non_survivors = all_data[all_data.hospital_expire_flag == 1]\n",
    "\n",
    "# CALCULATION OF THE PERCENT OF NON-SURVIVORS FOR WHICH EACH CONDITION IS TRUE AND FALSE\n",
    "for col in non_survivors.columns[1:]:\n",
    "    # NUMBER OF NON-SURVIVORS FOR WHICH CONDITION IS TRUE\n",
    "    dead_positive = non_survivors[col].sum()\n",
    "    total = non_survivors[col].count()\n",
    "    # PERCENTAGE OF NON-SURVIVORS FOR WHICH CONDITION IS FALSE AND TRUE\n",
    "    positive_percent = 100.0*dead_positive/total\n",
    "    negative_percent = 100.0*(total-dead_positive)/total\n",
    "    dead_positive_dict[col] = [positive_percent, negative_percent]\n",
    "    \n",
    "dead_positive_df =pd.DataFrame.from_dict(dead_positive_dict)\n",
    "dead_positive_df.index = [['Percent of Non_Survivors where Condition = True', 'Live']]\n",
    "#display(dead_positive_df)\n",
    "'''\n",
    "live_dead_df.transpose().plot.bar(stacked = True, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Non_Survivor Samples with Positive Label\")\n",
    "'''\n",
    "\n",
    "live_positive_dict = {}\n",
    "survivors = all_data[all_data.hospital_expire_flag == 0]\n",
    "\n",
    "# CALCULATION OF THE PERCENT OF SURVIVORS FOR WHICH EACH CONDITION IS TRUE AND FALSE\n",
    "for col in survivors.columns[1:]:\n",
    "    # NUMBER OF SURVIVORS FOR WHICH CONDITION IS TRUE\n",
    "    live_positive = survivors[col].sum()\n",
    "    total = survivors[col].count()\n",
    "    # PERCENT OF SURVIVORS FOR WHICH CONDITION IS TRUE AND FALSE\n",
    "    positive_percent = 100.0*live_positive/total\n",
    "    negative_percent = 100.0*(total-live_positive)/total\n",
    "    live_positive_dict[col] = [positive_percent, negative_percent]\n",
    "    \n",
    "live_positive_df=pd.DataFrame.from_dict(live_positive_dict)\n",
    "live_positive_df.index = [['Percent of Survivors for which Condition = True', 'Live2']]\n",
    "#display(live_positive_df)\n",
    "\n",
    "positive_df = dead_positive_df.append(live_positive_df)\n",
    "#display(positive_df[list(diagnoses_features.columns[1:])])\n",
    "# PLOTTING PERCENTAGES OF SURVIVORS AND NON-SURVIVORS FOR WHCIH CONDITION IS TRUE. \n",
    "# PLOTTING DIAGNOSES FEATURES IN ONE PLOT\n",
    "ax1 = positive_df[diagnoses_list[1:]].transpose()[['Percent of Non_Survivors where Condition = True','Percent of Survivors for which Condition = True']].plot.bar(stacked = False, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors and Non-Survivors for which Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "ax1.set_ylim([0,100]) \n",
    "#ax1.legend(loc=\"upper left\", bbox_to_anchor=(0.75,0.6),fontsize=12)\n",
    "# PLOTTING REST OF FEATURES IN ANOTHER PLOT\n",
    "ax2 = positive_df[meas_list[1:]].transpose()[['Percent of Non_Survivors where Condition = True','Percent of Survivors for which Condition = True']].plot.bar(stacked = False, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Percent of Survivors and Non-Survivors for which Condition is True\")\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "ax2.set_ylim([0,100])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CALCULATE SURVIVAL RATES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "total = all_data.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "# FOR SURVIVORS AND NON-SURVIVORS, CALCULATING THE PERCENTAGE OF ICU STAYS FOR \n",
    "# WHICH EACH CONDITION IS TRUE\n",
    "\n",
    "\n",
    "deads = all_data[all_data.hospital_expire_flag ==1].sum()\n",
    "deads.name = \"Number of True Values in Non-Survivors\"\n",
    "# COUNT THE NUMBER OF NON-SURVIVORS\n",
    "all_deads = all_data[all_data.hospital_expire_flag == 1].count()\n",
    "all_deads.name = \"Number of Non-Survivors\"\n",
    "\n",
    "# FOR SURVIVORS, COUNT THE NUMBER OF TRUE VALUES FOR EACH FEATURE\n",
    "survives = all_data[all_data.hospital_expire_flag == 0].sum()\n",
    "survives.name = \"Number of True Values in Survivors\"\n",
    "\n",
    "# COUNT THE TOTAL NUMBER OF SURVIVORS\n",
    "all_survives = all_data[all_data.hospital_expire_flag == 0].count()\n",
    "all_survives.name = \"Number of Surivor Patients\"\n",
    "\n",
    "dead_percent = 100.00*(deads / all_deads)\n",
    "dead_percent.name = \"% Non-Survivors for which Condition = True\"\n",
    "live_percent = 100.00*(survives / all_survives)\n",
    "live_percent.name = \"% Survivors for which Condition = True\"\n",
    "\n",
    "mortality_percents = pd.concat([live_percent, dead_percent], axis = 1)\n",
    "\n",
    "    #display(monkey)\n",
    "\n",
    "mortality_percents.rename(index=str, columns={\"Pneumonia (except that caused by tuberculosis or sexually transmitted disease)\": \"Pneumonia\",\n",
    "                                         \"Respiratory failure; insufficiency; arrest (adult)\": \"Respiratory Failure\"}, \n",
    "                     inplace = True)\n",
    "ax = mortality_percents.iloc[1:].plot.bar(stacked = False, figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Survival Rate for \" + col)\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "#ax.set_ylim([0,100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATING BENCHMARK FOR ALL CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CREATING BASELINE SCORES USING 20 INPUT FEATURES AND \n",
    "# CLASSIFIERS WITH DEFAULT SETTINGS\n",
    "\n",
    "SVC = svm.SVC()\n",
    "Kneighb = KNeighborsClassifier()\n",
    "LSVC = svm.LinearSVC()\n",
    "MLP = MLPClassifier()\n",
    "Tree = DecisionTreeClassifier() \n",
    "\n",
    "default_clfs = [SVC, Kneighb, LSVC, MLP, Tree]\n",
    "\n",
    "X_best = all_data[all_data.columns[1:]]\n",
    "y = all_data['hospital_expire_flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "first = True\n",
    "\n",
    "for clf in default_clfs:        \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    scores_report = metrics.classification_report(y_true, y_pred)\n",
    "    #print type(clf).__name__\n",
    "    #print scores_report\n",
    "            \n",
    "        \n",
    "    # CONVERT SCORES_REPORT TO DATAFRAME\n",
    "    scores = scores_report.split()\n",
    "    #display(scores)\n",
    "    cols = scores[:4]\n",
    "    #cols = ['default ' + x for x in cols]\n",
    "   \n",
    "    ind = ['Survivors', 'Non-Survivors', 'Avg/Total']\n",
    "    # CLEAN THIS UP\n",
    "    dat = [[float(x) for x in scores[5:9]], [float(x) for x in scores[10:14]], \n",
    "                                            [float(x) for x in scores[17:22]]]\n",
    "    \n",
    "    key = type(clf).__name__\n",
    "    #print key\n",
    "    arrays = [[key, key, key], ind]\n",
    "    tuples = list(zip(*arrays))\n",
    "    mindex = pd.MultiIndex.from_tuples(tuples, names=['Classifier', 'Classes'])\n",
    "    if first:\n",
    "        def_scores = pd.DataFrame(dat, columns = cols, index = mindex)\n",
    "        first = False\n",
    "    else: \n",
    "        def_scores = def_scores.append(pd.DataFrame(dat, columns = cols, index = mindex))\n",
    "        \n",
    "\n",
    "def_scores.name = \"Default Classifier Scores\"\n",
    "#def_scores.sort_index()\n",
    "display(def_scores.sort_index())\n",
    "#def_scores.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "monkey = def_scores.copy()\n",
    "classers = list(monkey.index.levels[0])\n",
    "metric_dict = {}\n",
    "\n",
    "monkey['f1 metric'] = 0.00\n",
    "monkey['recall metric'] = 0.00\n",
    "\n",
    "for clfr in classers:\n",
    "    f1survs = monkey['f1-score'][clfr, 'Survivors']\n",
    "    f1deads = monkey['f1-score'][clfr, 'Non-Survivors']\n",
    "    f1met = (f1survs+f1deads)-abs(f1survs-f1deads)\n",
    "    recall_survs = monkey['recall'][clfr, 'Survivors']\n",
    "    recall_deads = monkey['recall'][clfr, 'Non-Survivors']\n",
    "    recallmet = (recall_survs+recall_deads)-abs(recall_survs-recall_deads)\n",
    "    #display(recallmet)\n",
    "    metric_dict[clfr] = [f1met, recallmet]\n",
    "    monkey['f1 metric'][clfr, 'avg/total'] = f1met\n",
    "    monkey['recall metric'][clfr, 'avg/total'] = recallmet\n",
    "\n",
    "#display(monkey)\n",
    "bm_metric_df = pd.DataFrame.from_dict(metric_dict)\n",
    "bm_metric_df.index = ['F1 Metric', 'Recall Metric']\n",
    "bm_metric_df = bm_metric_df.transpose()\n",
    "display(bm_metric_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#ax1 = bl_metric_df['F1 Metric'].plot.bar(legend = True, figsize = (13,8), rot = 60)\n",
    "#ax2 = bl_metric_df['Recall Metric'].plot.bar(secondary_y=True, label = 'Recall Metric', legend = True, \n",
    "#                                         rot = 60)\n",
    "ax1 = bm_metric_df.plot.bar(legend = True, figsize = (13,8), rot = 60)\n",
    "ax1.set_ylim(0,1.5)\n",
    "#ax2.set_ylim(0,1.5)\n",
    "\n",
    "plt.title('Benchmark Optimization Metric Scores for Classifiers with Default Parameters\\n Trained and Tested on 20 Features')\n",
    "\n",
    "#plt.xticks(rotation = 60, ha = 'right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "## TRAINING THE SVC TAKES A VERY LONG TIME, ON THE ORDER OF HOURS THE OPTIMIZED PARAMETERS, ALONG WITH THOSE OF ADDITIONAL CLASSIFIERS HAVE BEEN PREVIOUSLY CALCULATED AND SAVED. BELOW IS CODE TO CALCULATE THESE VALUES. THESE VALUES HAVE BEEN PREVIOUSLY CALCULATED AND SAVED TO FILE. THIS CAN BE SKIPPED AND FURTHER DOWN IS CODE TO READ IN THOSE VALUE FROM FILE\n",
    "## XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "## OPTIMIZING SVC CLASSIFIER OVER PARAMETER AND FEATURE SPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE SUPPORT VECTOR CLASSIFIER IS OPTIMZED TO TO F1-SCORE AND RECALL OVER THE GIVEN PARAMETER SPACE\n",
    "# AND OVER THE NUMBER OF FEATURES USING THE TOP 20 FEATURES RANKED ACCORDING TO CHI2 SCORE\n",
    "# RECALL FOR MORTALITY PREDICTION IS IMPORTANT IN THAT WE DON'T WANT FALSE NEGATIVES. \n",
    "# THE SUPPORT VECTOR CLASSIFIER TAKES A VERY LONG TIME TO OPTIMIZE OVER THE GIVEN PARAMETER SPACE SO \n",
    "# THAT IS BEING DONE INDIVIDUALLY HERE. THE REMAINING CLASSIFIERS ARE OPTIMIZED BELOW\n",
    "\n",
    "optimized_clfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC CLASSIFIER PARAMETERES\n",
    "SVC_params = {'C':[0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9,1],  \n",
    "               'class_weight': [{1:3, 0:1}, {1:4, 0:1}, {1:5, 0:1}],\n",
    "               'kernel': ['rbf', 'sigmoid', 'poly',], \n",
    "               'degree': [2,3,4], \n",
    "               'decision_function_shape': ['ovr','ovo']\n",
    "                                \n",
    "              }\n",
    "\n",
    "\n",
    "# CLASSIFIER SCORES\n",
    "scores = ['f1', 'recall'] #'accuracy', , 'precision']\n",
    "\n",
    "# DICTIONARY OF CLASSIFIERS AND CORRESPONDING PARAMETERS \n",
    "SVC = svm.SVC(random_state = 42)\n",
    "SVC_stuff = [SVC, SVC_params]\n",
    "\n",
    "classifiers = {\n",
    "               'SVC' : SVC_stuff\n",
    "              }\n",
    "               \n",
    "\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    \n",
    "    recall_mortality = -100\n",
    "    f1_mortality = -100\n",
    "    optimized_params = {}\n",
    "    optimized = {}\n",
    "    optimized_scores = \"\"\n",
    "    recall_scores = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    print \"Evaluating {}\".format(key)\n",
    "    for num_feats in range(8, all_data.shape[1]):\n",
    "        print \"{} features\".format(num_feats)\n",
    "        # FEATURES AND TARGETS\n",
    "        X_best = all_data[all_data.columns[1:num_feats]]\n",
    "        y = all_data['hospital_expire_flag']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.30, random_state = 42)\n",
    "        \n",
    "        \n",
    "        # MAY NOT BE NECESSARY THE WAY THE CODE IS CURRENTLY BROKEN OUT\n",
    "        for score in scores:\n",
    "            #print\"# Optimizing parameters for {} classifier to {} score using {} features\".format(key, score, num_feats)\n",
    "\n",
    "         \n",
    "            clf = GridSearchCV(classifiers[key][0], classifiers[key][1], cv = 5, \n",
    "                           scoring = score)\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "    \n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            scores_report = metrics.classification_report(y_true, y_pred)\n",
    "            \n",
    "            \n",
    "            #print(clf.best_params_)\n",
    "            #print(scores_report)\n",
    "         \n",
    "            if score == 'recall':\n",
    "                surv, mort = metrics.recall_score(y_true, y_pred, average = None)\n",
    "                # a metric that maximizes both variables and minimizes the difference between them\n",
    "                recall_metric = mort + surv - (abs(mort-surv))\n",
    "                recall_scores[str(num_feats)] = [surv, mort, recall_metric]\n",
    "                if (recall_metric > recall_mortality):\n",
    "                    recall_mortality = recall_metric #math.sqrt(mort**2 + surv**2)\n",
    "                    optimized = clf.best_params_\n",
    "                    optimized['features'] = num_feats\n",
    "                    optimized_scores = scores_report\n",
    "                    recall_str = key + '_recall'\n",
    "                    if recall_str not in optimized_clfs:\n",
    "                        optimized_clfs[recall_str] = {}\n",
    "                    optimized_clfs[recall_str]['CLF'] = clf.best_estimator_\n",
    "                    optimized_clfs[recall_str]['PARAMS'] = optimized\n",
    "                    optimized_clfs[recall_str]['SCORES'] = optimized_scores\n",
    "                   \n",
    "         \n",
    "            elif score == 'f1':\n",
    "            \n",
    "                surv_f1, mort_f1 = metrics.f1_score(y_true, y_pred, average = None)\n",
    "                #a metric that maximizes both variables and minimizes the difference between them\n",
    "                f1_metric = surv_f1 + mort_f1 - (abs(surv_f1-mort_f1))\n",
    "                f1_scores[str(num_feats)] = [surv_f1, mort_f1, f1_metric]\n",
    "                if (f1_metric > f1_mortality):\n",
    "                    f1_mortality = f1_metric #math.sqrt(mort**2 + surv**2)\n",
    "                    #recall_delta = abs(mort-surv)\n",
    "                    f1_optimized = clf.best_params_\n",
    "                    f1_optimized['features'] = num_feats\n",
    "                    f1_optimized_scores = scores_report\n",
    "                    f1_str = key + '_f1'\n",
    "                    if f1_str not in optimized_clfs.keys():\n",
    "                        optimized_clfs[f1_str] = {}\n",
    "                    optimized_clfs[f1_str]['CLF'] = clf.best_estimator_\n",
    "                    optimized_clfs[f1_str]['PARAMS'] = f1_optimized\n",
    "                    optimized_clfs[f1_str]['SCORES'] = f1_optimized_scores\n",
    "                    \n",
    "\n",
    "    print \"ANALYSIS COMPLETE\"\n",
    "    recalls = pd.DataFrame.from_dict(recall_scores)\n",
    "    recalls = recalls.transpose()\n",
    "    recalls['features'] = recalls.index\n",
    "    recalls['features'] = recalls['features'].apply(lambda x: int(x))\n",
    "    recalls.sort_values(by = 'features', inplace = True)\n",
    "    recalls.columns = ['Survivors', 'Non-Survivors', 'Metric', 'Features']\n",
    "    recalls[['Survivors', 'Non-Survivors', 'Metric']].plot(title = 'Recall Scores', \n",
    "                                                           use_index = True, figsize = (16,6))\n",
    "    plt.show()\n",
    "    print \"Optimized Parameters for the {} Classifier for recall are\".format(key)\n",
    "    print(optimized)\n",
    "    print \"optimized Scores are\"\n",
    "    print(optimized_scores)\n",
    "       \n",
    "    \n",
    "    f1s = pd.DataFrame.from_dict(f1_scores)\n",
    "    f1s = f1s.transpose()\n",
    "    f1s['features'] = f1s.index\n",
    "    f1s['features'] = f1s['features'].apply(lambda x: int(x))\n",
    "    f1s.sort_values(by = 'features', inplace = True)\n",
    "    f1s.columns = ['Survivors', 'Non-Survivors', 'Metric', 'Features']\n",
    "    f1s[['Survivors', 'Non-Survivors', 'Metric']].plot(title = 'F1 Scores', use_index = True, figsize = (16,6))\n",
    "    plt.show()\n",
    "    print \"Optimized Parameters for the {} Classifier for f1_score are\".format(key)\n",
    "    print(f1_optimized)\n",
    "    print \"Optimized Scores are\"\n",
    "    print(f1_optimized_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZING CLASSIFIERS OVER PARAMETER SPACE AND FEATURE SET SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# OPTIMIZING THE KNEIGHBORS, LINEAR SUPPORT VECTOR AND MULTI LAYER PERCEPTRON CLASSIFIERS TO F1-SCORE AND RECALL\n",
    "# OVER THE GIVEN PARAMETER SPACE AND OVER THE NUMBER OF FEATURES USING THE TOP 20 FEATURES RANKED \n",
    "# ACCORDING TO CHI2 SCORE\n",
    "# RECALL FOR MORTALITY PREDICTION IS IMPORTANT IN THAT WE DON'T WANT FALSE NEGATIVES. \n",
    "\n",
    "# NEAREST NEIGHBOR CLASSIFIER PARAMETERS\n",
    "\n",
    "kneighbors_params = {'n_neighbors': [2,4,6,8,10,12], \n",
    "                     'weights': ['uniform', 'distance'], \n",
    "                     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                    }\n",
    "#LinearSVC CLASSIFIER PARAMETERES\n",
    "LSVC_params = {'C':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,0.8,0.9,1],  \n",
    "               'class_weight': [#{1:1, 0:1}, {1:2, 0:1}, {1:2.5, 0:1}, \n",
    "                                {1:3, 0:1}, {1:3.5, 0:1}, {1:4, 0:1}, \n",
    "                                {1:4.5, 0:1}, {1:5, 0:1}, {1:5.5, 0:1}, {1:6, 0:1}], \n",
    "                                #{1:6, 0:1}, {1:6.7, 0:1}, {1:7, 0:1}],\n",
    "                   'loss':['hinge', 'squared_hinge']\n",
    "              }\n",
    "\n",
    "\n",
    "MLP_params = {\n",
    "              'activation': ['identity', 'logistic', 'tanh', 'relu'], \n",
    "              'solver': ['lbfgs', 'sgd', 'adam']\n",
    "             \n",
    "             }\n",
    "\n",
    "Tree_params = {\n",
    "               'class_weight': [{1:1, 0:1}, {1:2, 0:1}, {1:2.5, 0:1}, \n",
    "                                {1:3, 0:1}, {1:3.5, 0:1}, {1:4, 0:1}, \n",
    "                                {1:4.5, 0:1}, {1:5, 0:1}, {1:5.5, 0:1}, {1:6, 0:1}],\n",
    "                'criterion': ['gini', 'entropy']\n",
    "              }\n",
    "\n",
    "\n",
    "\n",
    "# DICTIONARY OF CLASSIFIERS AND CORRESPONDING PARAMETERS \n",
    "\n",
    "\n",
    "# CLASSIFIER SCORES\n",
    "scores = ['f1', 'recall'] #'accuracy',, 'recall' , 'precision']\n",
    "\n",
    "# DICTIONARY OF CLASSIFIERS AND CORRESPONDING PARAMETERS \n",
    "Kneighb = KNeighborsClassifier()\n",
    "LSVC = svm.LinearSVC(random_state = 42)\n",
    "MLP = MLPClassifier(random_state = 42)\n",
    "Tree = DecisionTreeClassifier(random_state = 42) \n",
    "#SVC = svm.SVC(random_state = 42)\n",
    "LSVC_stuff = [LSVC, LSVC_params]\n",
    "KNeighbors_stuff = [Kneighb, kneighbors_params]\n",
    "MLP_stuff = [MLP, MLP_params]\n",
    "Tree_stuff = [Tree, Tree_params]\n",
    "\n",
    "#SVC_stuff = [SVC, SVC_params]\n",
    "classifiers = {'LSVC': LSVC_stuff,\n",
    "               'Kneighbors': KNeighbors_stuff, \n",
    "               'MLP': MLP_stuff,\n",
    "               'Tree': Tree_stuff\n",
    "               \n",
    "              }\n",
    "                \n",
    "\n",
    "\n",
    "for key in classifiers.keys():\n",
    "    \n",
    "    recall_mortality = -100000\n",
    "    f1_mortality = -100000\n",
    "    optimized_params = {}\n",
    "    optimized = {}\n",
    "    \n",
    "    optimized_scores = \"\"\n",
    "    recall_scores = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    print \"Evaluating {}\".format(key)\n",
    "    for num_feats in range(8, all_data.shape[1]):\n",
    "        print \"{} features\".format(num_feats)\n",
    "        # FEATURES AND TARGETS\n",
    "        X_best = all_data[all_data.columns[1:(num_feats+1)]]\n",
    "        y = all_data['hospital_expire_flag']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.30, random_state = 42)\n",
    "        \n",
    "        if key == 'MLP':\n",
    "            input_layer = num_feats\n",
    "            middle_layer = int(math.ceil(num_feats/2))\n",
    "            classifiers[key][0] = MLPClassifier(random_state = 42, \n",
    "                                                hidden_layer_sizes = (input_layer, middle_layer)\n",
    "                                                )\n",
    "            print \"MLP Layers ({}, {})\".format(num_feats, math.ceil(num_feats/2))\n",
    "        # MAY NOT BE NECESSARY THE WAY THE CODE IS CURRENTLY BROKEN OUT\n",
    "        for score in scores:\n",
    "            #print\"# Optimizing parameters for {} classifier to {} score using {} features\".format(key, score, num_feats)\n",
    "\n",
    "         \n",
    "            clf = GridSearchCV(classifiers[key][0], classifiers[key][1], cv = 5, \n",
    "                           scoring = score)\n",
    "\n",
    "            clf.fit(X_train, y_train)\n",
    "    \n",
    "            y_true, y_pred = y_test, clf.predict(X_test)\n",
    "            scores_report = metrics.classification_report(y_true, y_pred)\n",
    "            #print(scores_report)\n",
    "         \n",
    "            if score == 'recall':\n",
    "                surv, mort = metrics.recall_score(y_true, y_pred, average = None)\n",
    "                # a metric that maximizes both variables and minimizes the difference between them\n",
    "                recall_metric = mort + surv - (abs(mort-surv))\n",
    "                recall_scores[str(num_feats)] = [surv, mort, recall_metric]\n",
    "                if (recall_metric > recall_mortality):\n",
    "                    recall_mortality = recall_metric \n",
    "                    optimized = clf.best_params_\n",
    "                    optimized['features'] = num_feats\n",
    "                    optimized_scores = scores_report\n",
    "                    recall_str = key + '_recall'\n",
    "                    if recall_str not in optimized_clfs:\n",
    "                        optimized_clfs[recall_str] = {}\n",
    "                    optimized_clfs[recall_str]['CLF'] = clf.best_estimator_\n",
    "                    optimized_clfs[recall_str]['PARAMS'] = optimized\n",
    "                    optimized_clfs[recall_str]['SCORES'] = optimized_scores\n",
    "                    #print \"OPTIMIZED RECALL VALUES REGISTERED\"\n",
    "                    \n",
    "         \n",
    "            elif score == 'f1':\n",
    "            \n",
    "                surv_f1, mort_f1 = metrics.f1_score(y_true, y_pred, average = None)\n",
    "                #a metric that maximizes both variables and minimizes the difference between them\n",
    "                f1_metric = surv_f1 + mort_f1 - (abs(surv_f1-mort_f1))\n",
    "                f1_scores[str(num_feats)] = [surv_f1, mort_f1, f1_metric]\n",
    "                if (f1_metric > f1_mortality):\n",
    "                    f1_mortality = f1_metric #math.sqrt(mort**2 + surv**2)\n",
    "                    #recall_delta = abs(mort-surv)\n",
    "                    f1_optimized = clf.best_params_\n",
    "                    f1_optimized['features'] = num_feats\n",
    "                    f1_optimized_scores = scores_report\n",
    "                    f1_str = key + '_f1'\n",
    "                    if f1_str not in optimized_clfs:\n",
    "                        optimized_clfs[f1_str] = {}\n",
    "                    optimized_clfs[f1_str]['CLF'] = clf.best_estimator_\n",
    "                    optimized_clfs[f1_str]['PARAMS'] = f1_optimized\n",
    "                    optimized_clfs[f1_str]['SCORES'] = f1_optimized_scores\n",
    "                    #print \"OPTIMIZED F1 VALUES REGISTERED\"\n",
    "                    \n",
    "            \n",
    "   \n",
    "    print \"ANALYSIS COMPLETE\"\n",
    "    recalls = pd.DataFrame.from_dict(recall_scores)\n",
    "    recalls = recalls.transpose()\n",
    "    recalls['features'] = recalls.index\n",
    "    recalls['features'] = recalls['features'].apply(lambda x: float(x))\n",
    "    recalls.sort_values(by = 'features', inplace = True)\n",
    "    recalls.columns = ['Survivors', 'Non-Survivors', 'Metric', 'Features']\n",
    "    recalls[['Survivors', 'Non-Survivors', 'Metric']].plot(title = 'Recall Scores', \n",
    "                                                           use_index = True, figsize = (16,6))\n",
    "    plt.show()\n",
    "    print \"Optimized Parameters for recall are\"\n",
    "    print(optimized)\n",
    "    print \"optimized Scores are\"\n",
    "    print(optimized_scores)\n",
    "        \n",
    "    \n",
    "    f1s = pd.DataFrame.from_dict(f1_scores)\n",
    "    f1s = f1s.transpose()\n",
    "    f1s['features'] = f1s.index\n",
    "    f1s['features'] = f1s['features'].apply(lambda x: float(x))\n",
    "    f1s.sort_values(by = 'features', inplace = True)\n",
    "    f1s.columns = ['Survivors', 'Non-Survivors', 'Metric', 'Features']\n",
    "    f1s[['Survivors', 'Non-Survivors', 'Metric']].plot(title = 'F1 Scores', use_index = True, figsize = (16,6))\n",
    "    plt.show()\n",
    "    print \"Optimized Parameters for f1 are\"\n",
    "    print(f1_optimized)\n",
    "    print \"Optimized Scores are\"\n",
    "    print(f1_optimized_scores)\n",
    "    \n",
    "print \"ANALYSIS COMPLETE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFIERS WITH OPTIMIZED PARAMETERS OVER VARIOUS TRAIN-TEST-SPLIT SIZES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"monkey\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# TRAINING AND TESTING CLASSIFIERS PREVIOUSLY OPTIMIZED OVER PARAMETER SPACE AND FEATURES \n",
    "# OVER A RANGE OF TRAIN TEST SPLIT SIZE. \n",
    "\n",
    "for key in optimized_clfs.keys():\n",
    "    \n",
    "    recall_mortality = 0\n",
    "    f1_mortality = 0\n",
    "    optimized_params = {}\n",
    "    optimized = {}\n",
    "    optimized_scores = \"\"\n",
    "    recall_scores = {}\n",
    "    f1_scores = {}\n",
    "\n",
    "    print \"Evaluating {}\".format(key)\n",
    "    for testes_size in np.linspace(0.1, 0.5, 9):\n",
    "        # DEFINING MLP HERE SO THAT WE CAN ALTER THE NUMBER OF INPUT HIDDEN NODES TO MATCH FEATURES\n",
    "        \n",
    "        #print \"{} test size\".format(testes_size)\n",
    "        # FEATURES AND TARGETS\n",
    "        features = optimized_clfs[key]['PARAMS']['features']\n",
    "        clf = optimized_clfs[key]['CLF']\n",
    "        X_best = all_data[all_data.columns[1:features]]\n",
    "        y = all_data['hospital_expire_flag']\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = testes_size, random_state = 42)\n",
    "        \n",
    "        \n",
    "        \n",
    "        clf.fit(X_train, y_train)\n",
    "    \n",
    "        y_true, y_pred = y_test, clf.predict(X_test)\n",
    "        scores_report = metrics.classification_report(y_true, y_pred)\n",
    "            #print(scores_report)\n",
    "         \n",
    "        if 'recall' in key:\n",
    "            surv, mort = metrics.recall_score(y_true, y_pred, average = None)\n",
    "                # a metric that maximizes both variables and minimizes the difference between them\n",
    "            recall_metric = mort + surv - (abs(mort-surv))\n",
    "            recall_scores[str(testes_size)] = [surv, mort, recall_metric]\n",
    "            if (recall_metric > recall_mortality):\n",
    "                recall_mortality = recall_metric #math.sqrt(mort**2 + surv**2)\n",
    "                optimized = optimized_clfs[key]['PARAMS']\n",
    "                optimized['test_size'] = testes_size\n",
    "                optimized_clfs[key]['PARAMS']['test_size'] = testes_size\n",
    "                optimized_clfs[key]['SCORES'] = scores_report\n",
    "                optimized_scores = scores_report\n",
    "                \n",
    "         \n",
    "        elif 'f1' in key:\n",
    "                \n",
    "            surv_f1, mort_f1 = metrics.f1_score(y_true, y_pred, average = None)\n",
    "                #a metric that maximizes both variables and minimizes the difference between them\n",
    "            f1_metric = surv_f1 + mort_f1 - (abs(surv_f1-mort_f1))\n",
    "            f1_scores[str(testes_size)] = [surv_f1, mort_f1, f1_metric]\n",
    "            if (f1_metric > f1_mortality):\n",
    "                f1_mortality = f1_metric #math.sqrt(mort**2 + surv**2)\n",
    "                #recall_delta = abs(mort-surv)\n",
    "                f1_optimized = optimized_clfs[key]['PARAMS']\n",
    "                f1_optimized['test_size'] = testes_size\n",
    "                optimized_clfs[key]['PARAMS']['test_size'] = testes_size\n",
    "                optimized_clfs[key]['SCORES'] = scores_report\n",
    "                f1_optimized_scores = scores_report\n",
    "                \n",
    "            \n",
    "    if 'recall' in key:   \n",
    "        print \"ANALYSIS COMPLETE\"\n",
    "        recalls = pd.DataFrame.from_dict(recall_scores)\n",
    "        recalls = recalls.transpose()\n",
    "        recalls['test_size'] = recalls.index\n",
    "        recalls['test_size'] = recalls['test_size'].apply(lambda x: float(x))\n",
    "        recalls.sort_values(by = 'test_size', inplace = True)\n",
    "        recalls.columns = ['Survivors', 'Non-Survivors', 'Metric', 'test_size']\n",
    "        recalls[['Survivors', 'Non-Survivors', 'Metric']].plot(title = key + ': Recall Scores', \n",
    "                                                           use_index = True, figsize = (16,6))\n",
    "        plt.show()\n",
    "        print \"Optimized Parameters for {} recall-score are\".format(key)\n",
    "        print(optimized)\n",
    "        print \"optimized Scores are\"\n",
    "        print(optimized_scores)\n",
    "    \n",
    "    \n",
    "    elif 'f1' in key:\n",
    "        f1s = pd.DataFrame.from_dict(f1_scores)\n",
    "        f1s = f1s.transpose()\n",
    "        f1s['test_size'] = f1s.index\n",
    "        f1s['test_size'] = f1s['test_size'].apply(lambda x: float(x))\n",
    "        f1s.sort_values(by = 'test_size', inplace = True)\n",
    "        f1s.columns = ['Survivors', 'Non-Survivors', 'Metric', 'test_Size']\n",
    "        f1s[['Survivors', 'Non-Survivors', 'Metric']].plot(title = key + ': F1 Scores', use_index = True, figsize = (16,6))\n",
    "        plt.show()\n",
    "        print \"Optimized Parameters for {} f1-score are\".format(key)\n",
    "        print(f1_optimized)\n",
    "        print \"Optimized Scores are\"\n",
    "        print(f1_optimized_scores)\n",
    "    \n",
    "print \"OPTIMIZATION COMPLETE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITING OPTIMIZED PARAMETERS FOR ALL CLASSIFIERS TO FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "with open('Optimized_Classifiers.txt', 'w') as file:\n",
    "    file.write(pickle.dumps(optimized_clfs))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "## READING IN PREVIOUSLY CALCULATED OPTIMIZED PARAMETERS. UNCOMMENT OUT CODE AND RUN TO READ IN OPTIMIZED CLASSIFIERS, PARAMETERS AND SCORES\n",
    "## XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# READ IN PREVIOUSLY CALCULATED OPTIMIZED PARAMETERS\n",
    "optimized_clfs = pickle.load(open('Optimized_Classifiers.txt', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CREATE DATAFRAME FROM SCORE REPORTS FOR OPTIMIZED CLASSIFERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CREATE DATAFRAME FROM SCORE REPORTS FOR OPTIMIZED CLASSIFIERS\n",
    "\n",
    "first = True\n",
    "for key in optimized_clfs.keys():\n",
    "    print(key)\n",
    "    #print(optimized_clfs[key]['SCORES'])\n",
    "\n",
    "    # CONVERT SCORES_REPORT TO DATAFRAME\n",
    "    scores = optimized_clfs[key]['SCORES'].split()\n",
    "    #display(scores)\n",
    "    cols = scores[:4]\n",
    "    '''\n",
    "    ind = [x for x in scores if scores.index(x) in [4,9,14, 15, 16]]\n",
    "    ind[2] = ind[2] + ind[3] + ind[4] \n",
    "    ind.pop()\n",
    "    ind.pop()\n",
    "    '''\n",
    "    ind = ['Survivors', 'Non-Survivors', 'Avg/Total']\n",
    "    \n",
    "    # CLEAN THIS UP\n",
    "    dat = [[float(x) for x in scores[5:9]], [float(x) for x in scores[10:14]], \n",
    "                                            [float(x) for x in scores[17:22]]]\n",
    "    #display(cols)\n",
    "    #display(ind)\n",
    "    #display(dat)\n",
    "    \n",
    "    arrays = [[key, key, key], ind]\n",
    "    tuples = list(zip(*arrays))\n",
    "    mindex = pd.MultiIndex.from_tuples(tuples, names=['Classifier', 'Classes'])\n",
    "    if first:\n",
    "        scores_frame = pd.DataFrame(dat, columns = cols, index = mindex)\n",
    "        first = False\n",
    "    else: \n",
    "        scores_frame = scores_frame.append(pd.DataFrame(dat, columns = cols, index = mindex))\n",
    "\n",
    "scores_frame.sort_index(level = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITING OPTIMIZED SCORES TO FILE\n",
    "scores_frame.to_csv('Optimized_Classifier_Scores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "display(scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Avg/Total'])\n",
    "display(scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Survivors'])\n",
    "display(scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Non-Survivors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "monkey = scores_frame.copy()\n",
    "classers = list(monkey.index.levels[0])\n",
    "metric_dict = {}\n",
    "\n",
    "monkey['f1 metric'] = 0.00\n",
    "monkey['recall metric'] = 0.00\n",
    "\n",
    "for clfr in classers:\n",
    "    f1survs = monkey['f1-score'][clfr, 'Survivors']\n",
    "    f1deads = monkey['f1-score'][clfr, 'Non-Survivors']\n",
    "    f1met = (f1survs+f1deads)-abs(f1survs-f1deads)\n",
    "    recall_survs = monkey['recall'][clfr, 'Survivors']\n",
    "    recall_deads = monkey['recall'][clfr, 'Non-Survivors']\n",
    "    recallmet = (recall_survs+recall_deads)-abs(recall_survs-recall_deads)\n",
    "    #display(recallmet)\n",
    "    metric_dict[clfr] = [f1met, recallmet]\n",
    "    monkey['f1 metric'][clfr, 'avg/total'] = f1met\n",
    "    monkey['recall metric'][clfr, 'avg/total'] = recallmet\n",
    "\n",
    "#display(monkey)\n",
    "metric_df = pd.DataFrame.from_dict(metric_dict)\n",
    "metric_df.index = ['F1 Metric', 'Recall Metric']\n",
    "metric_df = metric_df.transpose()\n",
    "display(metric_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax1 = metric_df.plot.bar(legend = True, figsize = (13,8), rot = 60)\n",
    "#ax2 = metric_df['Recall Metric'].plot(secondary_y=True, label = 'Recall Metric', legend = True, rot = 60)\n",
    "\n",
    "plt.title('Optimization Metric Scores for Classifiers with Optimized Parameters\\n Feature Set Size and Train/Test Splits')\n",
    "\n",
    "\n",
    "#plt.title('Optimization Metric Scores for Optimized Classifiers')\n",
    "\n",
    "#plt.xticks(rotation = 60, ha = 'right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "ax = scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Avg/Total']\\\n",
    "    [['precision', 'recall', 'f1-score']].plot(\n",
    "    title = 'AVG/Total Values', use_index = True, figsize= (25,16))\n",
    "ax.set_ylim([0.0,1.0])\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "plt.title('Average Precision, Recall and F1-Scores for Optimized Classifiers')\n",
    "    \n",
    "ax2 = scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Survivors']\\\n",
    "[['precision', 'recall', 'f1-score']].plot(\n",
    "    title = 'Survivor Values', use_index = True, figsize= (25,16))\n",
    "ax2.set_ylim([0.0,1.0])\n",
    "for item in ([ax2.title, ax2.xaxis.label, ax2.yaxis.label] +\n",
    "             ax2.get_xticklabels() + ax2.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "plt.title('Survivor Precision, Recall and F1-Scores for Optimized Classifiers')   \n",
    "\n",
    "\n",
    "ax3 = scores_frame.iloc[scores_frame.index.get_level_values('Classes') == 'Non-Survivors']\\\n",
    "[['precision', 'recall', 'f1-score']].plot(\n",
    "    title = 'Non-Survivor Values', use_index = True, figsize= (25,16))\n",
    "ax3.set_ylim([0.0,1.0])\n",
    "for item in ([ax3.title, ax3.xaxis.label, ax3.yaxis.label] +\n",
    "             ax3.get_xticklabels() + ax3.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "plt.xticks(rotation = 60, ha = 'right')\n",
    "plt.title('Non-Survivor Precision, Recall and F1-Scores for Optimized Classifiers')\n",
    "#recalls[['Survivors', 'Non-Survivors', 'Metric']].plot(title = key + ': Recall Scores', \n",
    "#                                                           use_index = True, figsize = (16,6))\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "scores_frame.loc[scores_frame.index.get_level_values('Classifier') == 'LSVC_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "type(Tree).__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# CALCULATING CROSS VALIDATION SCORES FOR HIGHEST PERFORMING BENCHMARK CLASSIFIER AND \n",
    "# HIGHEST PERFORMING OPTIMIZED CLASSIFIER\n",
    "\n",
    "cross_val_dict = {}\n",
    "\n",
    "print \"Benchmark Decision-Tree Classifier Performance\"\n",
    "Tree = DecisionTreeClassifier() \n",
    "X_default = all_data[all_data.columns[1:]]\n",
    "y_default = all_data['hospital_expire_flag']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.20, random_state = 42)\n",
    "\n",
    "#print \"Cross validation score on f1 (Non-Survivors Only)\"\n",
    "default_scores = cross_val_score(Tree, X_train, y_train, cv = 5, scoring = 'f1')\n",
    "cross_val_dict['F1 Non-Surivors: Default DecisionTree'] = default_scores\n",
    "#display(default_scores) \n",
    "#print \"Cross validation score on f1 (Average for Survivors and Non-Survivors)\"\n",
    "default_scores = cross_val_score(Tree, X_train, y_train, cv = 5, scoring = 'f1_macro')\n",
    "cross_val_dict['F1 Average: Default DecisionTree'] = default_scores\n",
    "#display(default_scores) \n",
    "\n",
    "#print \"Cross validation score on recall (Non-Survivors Only)\"\n",
    "default_scores = cross_val_score(Tree, X_train, y_train, cv = 5, scoring = 'recall')\n",
    "cross_val_dict['Recall Non-Surivors: Default DecisionTree'] = default_scores\n",
    "#display(default_scores)\n",
    "#print \"Cross validation score on recall (Average for Survivors and Non-Survivors)\"\n",
    "default_scores = cross_val_score(Tree, X_train, y_train, cv = 5, scoring = 'recall_macro')\n",
    "cross_val_dict['Recall Average: Default DecisionTree '] = default_scores\n",
    "#display(default_scores)\n",
    "\n",
    "                                \n",
    "                                \n",
    "#print \"Tree fit and test score\"\n",
    "\n",
    "#display(Tree.score(X_test, y_test))\n",
    "Tree.fit(X_train, y_train)\n",
    "print \"Tree Confusion Matrix and Classification Report\"\n",
    "y_preds = Tree.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_preds))\n",
    "print metrics.classification_report(y_test, y_preds)\n",
    "\n",
    "\n",
    "# PERFORMING CROSS VALIDATION ON LSVC_RECALL WITH OPTIMIZED PARAMETERS\n",
    "print \"Optimized LSVC Classifier Performance\"\n",
    "X_best = all_data[all_data.columns[1:11]]\n",
    "y = all_data['hospital_expire_flag']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_best, y, test_size = 0.50, random_state = 42)\n",
    "\n",
    "clf = optimized_clfs['LSVC_recall']['CLF']#.best_estimator_\n",
    "\n",
    "#print \"Cross validation score on f1 (Non-Survivors Only)\"\n",
    "optimized_scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'f1')\n",
    "cross_val_dict['F1 Non-Surivors: Optimized LSVC'] = optimized_scores\n",
    "#display(optimized_scores)\n",
    "#print \"Cross validation score on f1 (Average for Survivors and Non-Survivors)\"\n",
    "optimized_scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'f1_macro')\n",
    "cross_val_dict['F1 Average: Optimized LSVC'] = optimized_scores\n",
    "#display(optimized_scores)\n",
    "\n",
    "#print \"Cross validation score on recall (Non-Survivors Only)\"\n",
    "optimized_scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'recall')\n",
    "cross_val_dict['Recall Non-Survivors: Optimized LSVC'] = optimized_scores\n",
    "#display(optimized_scores)\n",
    "#print \"Cross validation score on recall (Average for Survivors and Non-Survivors)\"\n",
    "optimized_scores = cross_val_score(clf, X_train, y_train, cv = 5, scoring = 'recall_macro')\n",
    "cross_val_dict['Recall Average: Optimized LSVC'] = optimized_scores\n",
    "#display(optimized_scores)\n",
    "\n",
    "\n",
    "#print \"LSVC fit test\"\n",
    "#display(scores)  \n",
    "\n",
    "#display(clf.score(X_test, y_test))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "print \"LSVC Confusion Matrix and Classification Report\"\n",
    "y_preds = clf.predict(X_test)\n",
    "display(metrics.confusion_matrix(y_test, y_preds))\n",
    "print metrics.classification_report(y_test, y_preds)\n",
    "\n",
    "cross_val_df = pd.DataFrame.from_dict(cross_val_dict)\n",
    "cross_cols = list(cross_val_df.columns)\n",
    "cross_cols.sort()\n",
    "cross_val_F1_df = cross_val_df[cross_cols[:4]]\n",
    "cross_val_Recall_df = cross_val_df[cross_cols[4:]]\n",
    "display(cross_val_F1_df.round(2))\n",
    "display(cross_val_Recall_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# CREATE DICTIONARIES FOR CROSS VALIDATION STATISTICS. WILL LATER BE CONVERTED TO DATAFRAMES\n",
    "\n",
    "cross_val_F1_stats_dict = {}\n",
    "cross_val_Recall_stats_dict = {}\n",
    "cross_val_F1_stats_dict['F1 Means'] = cross_val_F1_df.mean()\n",
    "cross_val_F1_stats_dict['F1 Stds'] = cross_val_F1_df.std()\n",
    "\n",
    "\n",
    "# CALCULATE STATISTICS\n",
    "cross_val_Recall_stats_dict['Recall Means'] = cross_val_Recall_df.mean()\n",
    "cross_val_Recall_stats_dict['Recall Stds'] = cross_val_Recall_df.std()\n",
    "cross_val_F1_stats_df = pd.DataFrame.from_dict(cross_val_F1_stats_dict)\n",
    "cross_val_Recall_stats_df = pd.DataFrame.from_dict(cross_val_Recall_stats_dict)\n",
    "display(cross_val_F1_stats_df)\n",
    "display(cross_val_Recall_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cross_val_F1_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cross_val_F1_stats_df.plot.bar(figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Cross Validation F1 Scores: \\n Average Scores and Standard Deviation for Default Decision Tree and Optimized LSVC Classifiers\")\n",
    "\n",
    "plt.xticks(rotation = 60, ha = 'right')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "cross_val_Recall_stats_df.plot.bar(figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"Cross Validation Recall Scores: \\nAverage Standard Deviation for Default Decision Tree and Optimized LSVC Classifiers\")\n",
    "\n",
    "plt.xticks(rotation = 60, ha = 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "LSVC_df = scores_frame.loc[scores_frame.index.get_level_values('Classifier') == 'LSVC_recall']\n",
    "LSVC_DTC_df = LSVC_df.append(def_scores.loc[def_scores.index.get_level_values('Classifier') == 'DecisionTreeClassifier'])\n",
    "\n",
    "\n",
    "LSVC_DTC_df[['f1-score', 'recall']].plot.bar(figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"F1 and Recall Scores for Default Decision Tree and Optimized LSVC Classifiers\")\n",
    "\n",
    "plt.ylim(0,1)\n",
    "plt.xticks(rotation = 60, ha = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLOT DEFAULT VS OPTIMIZED LSVC CLASSIFIER SCORES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "LSVC_df2 = scores_frame.loc[scores_frame.index.get_level_values('Classifier') == 'LSVC_recall']\n",
    "LSVC_df2 = LSVC_df2.append(def_scores.loc[def_scores.index.get_level_values('Classifier') == 'LinearSVC'])\n",
    "\n",
    "ind = ['Survivors', 'Non-Survivors', 'Avg/Total']\n",
    "    # CLEAN THIS UP\n",
    "arrays = [['Optimized LSVC', 'Optimized LSVC', 'Optimized LSVC',\n",
    "               'Default LSVC','Default LSVC','Default LSVC'], \n",
    "              ['Survivors', 'Non-Survivors', 'Avg/Total', 'Survivors', 'Non-Survivors', 'Avg/Total']]\n",
    "    \n",
    "mindex = pd.MultiIndex.from_arrays(arrays, names=['Classifier', 'Classes'])\n",
    "\n",
    "monkey_df = LSVC_df2.copy()\n",
    "\n",
    "LSVC_df3 = monkey_df.set_index(arrays)\n",
    "\n",
    "LSVC_df3[['f1-score', 'recall']].plot.bar(figsize = (13,6), edgecolor = 'black', linewidth = 3, \n",
    "                                alpha = 0.5, title = \"F1 and Recall Scores for Default and Optimized LinearSVC Classifiers\")\n",
    "\n",
    "\n",
    "plt.xticks(rotation = 60, ha = 'right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *fin*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
